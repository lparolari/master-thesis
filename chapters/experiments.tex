%!TEX root = ../dissertation.tex
\begin{savequote}[75mm]
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname}
\end{savequote}

\chapter{Experiment and results}

SCALETTA

* datasets
  * flickr
  * referit
* evaluation metric
* impl details
* model selection
* results
* qualitative results

\todo{CITE \cite{schmitt2019replicable} in SpaCy impl. details}

\section{Datasets}
\label{sec:datasets}

In this section we describe Flickr30K and ReferIt, the two datasets
used for the experimental assessment. \todo{??? Furthermore, we
describe two other noteworthy and popular datasets available in
literature}

\subsection{Flickr30K}
\label{subsec:flickr30k}

Flickr30K Entities \todo{CITE: Flickr30K Entities} is a dataset built
on top of Flickr30K \todo{CITE: From image descriptions to visual
denotations: New similarity metrics for semantic inference over event
descriptions}. It is usually referred as Flickr30K or simply as Flickr
and, compared to the original Flickr30K dataset, it focuses on the
task of grounding textual mentions of entities in image by augmenting
data with extra annotations.

Annotations consists of cross-caption coreference chains linking
mentions of the same entities together with bounding boxes localizing
those entities in the image, and are collected throught a
crowdsourcing protocol. These annotations are highly structured and
vary in complexity from image to image, since images vary in the
numbers of clearly distinguishable entities they contain, and
sentences vary in the extent of their detail. Further, there are
ambiguities involved in identifying whether two mentions refer to the
same entity or set of entities, how many boxes (if any) these entities
require, and whether these boxes are of sufficiently high quality. To
tackle this problem, compounded by the unreliability of crowdsourced
judgments, they administer a pipeline of simple and atomic tasks which
can be grouped into two main stages: coreference resolution, or
forming coreference chains that refer to the same entities, and
bounding box annotation for the resulting chains. This workflow
provides two advantages: first, identifying coreferent mentions helps
reduce redundancy and save box-drawing effort; and second, coreference
annotation is intrinsically valuable. In
Fig.~\ref{fig:flickr30k-example}, taken from \todo{CITE: flickr30k
entitites} are shown the interfaces used to gather annotations with
respect to tasks described next.

\begin{figure}
  \centering
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{figures/flickr30k-example-coref-annotation.png}
    \caption{Binary coreference link interface}
    \label{fig:flickr30k-example-coref-annotation}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{figures/flickr30k-example-coref-verification.png}
    \caption{Coreference chain verification interface}
    \label{fig:flickr30k-example-coref-verification}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{figures/flickr30k-example-box-requirement.png}
    \caption{Box requirement interface}
    \label{fig:flickr30k-example-box-requirement}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{figures/flickr30k-example-box-drawing.png}
    \caption{Box drawing interface}
    \label{fig:flickr30k-example-box-drawing}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{figures/flickr30k-example-box-quality.png}
    \caption{Box quality interface}
    \label{fig:flickr30k-example-box-quality}
  \end{subfigure}
  \begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{figures/flickr30k-example-box-coverage.png}
    \caption{Box coverage interface}
    \label{fig:flickr30k-example-box-coverage}
  \end{subfigure}
  \caption{Examples of the interfaces used in TODO CITE FLICKR annotation system.}
  \label{fig:flickr30k-example}
\end{figure}

The coreference resolution task is solved by first chunking
information given in Flickr captions to identify potential entity
mentions. Each chunk, i.e., noun phrase (NP), is a potential entity
mention. For example, the frase ``A man in an orange hat'' is chunked
in two noun phrases, namely ``A man'' and ``an orange hat'', which are
the two mentions. Given $M$ a document containing noun phrases
originated from captions of an image, a worker is required to specify
whether two given mentions $m$ and $m'$ refer to the same entity. If
the answer is positive, a link between the two mentions is addded,
creating a coreference chain. Tipically, this would require $O(|M|^2)$
which is the cost of all pairwise links. But since $M$ usually
contains multiple mentions that refer to the same set of entities, the
number of coreference chains is bounded by a number much smaller than
$|M|$, on average. Also, the bound is lowered by making two assumptions.
First, they assume that mentions from the same captions cannot be
coreferent; second, they categorize each mention into eight
coarse-grained types using manually constructed dictionaries (people,
body parts, animals, clothing/color, instruments, vehicles, scene, and
other), and assume mentions belonging to different categories cannot
be coreferent. The goodnees of the strategy is empirically shown by a
small-scale test on 200 images and the coreference chains are verified
by another task which asks to a worker whether given mentions in
coreference chain refer to the same entity. Their results shows that
only $9.8\%$ of all coreference chains are not correct.

Bounding box annotation is carried out with a workflow composed by
four tasks administered to workers, namely: box requirement, box
drawing, box quality and box coverage. In box requirement, a worker
needs to specify, whether at least one bounding box can be drawn in
image for given mention. If the response is negative, the mention
leaves the workflow, otherwise it goes throught the box drawing stage.
Here, a worker should draw a box as tight as possible around the
mentioned entity. The main source of difficulty in this stage is due
to mentions that refer to multiple entities. If a box is drawn, then
the mention-box pair proceeds to box quality stage. At this point,
drawn box is evaluated in terms of redundancy (are there any box that
already covers the same entity?), accuracy (is the box tight around
the entity?), distinctiveness (can a box be drawn for single entity
instead of covering multiple elements?). If the answer is positive,
the example goes to the last stage. In box coverage, workers decide
whether all required boxes are present in image. 

During the annotation process, workers are subjected to quality
control process. Workers must pass a test before being allowed to
annotate dataset examples. Also, during the annotation process,
workers are asked some verification questions (question with known
answer, written by authors).

In the end, Flicker30K Entities is made up of 32K images, 159K
sentences, 275K bounding boxes, and 360K noun phrases where each image
is associated with five sentences with a variable number of noun
phrases, and each noun phrase is associated with a set of bounding
boxes ground truth coordinates. 

\subsubsection{Data Representation}
\label{subsec:flickr30k-data-representation}

Technically speaking, in Flickr30K Entities each example is composed
by information contained in three different files linked together by
means of an identifier $id$. Those files represent image, sentence
annotations and image annotations information. The image file is
simply a JPG file, while the sentence annotations file is a textual
document containing five captions. Each caption is chunked into a
variable number of noun phrases and each noun phrase can be linked to
a bounding box contained in the third file. The image annotation file
is an XML file with an array of objects. Every object has one or more
name, that can be used to reconstruct which noun phrase is linked to
the bounding box, and four coordinates $x_{min}, y_{min}, x_{max},
y_{max}$ if it is a visual element, none otherwise. The structure is
visually explained in
Fig.~\ref{fig:flickr30k-technical-data-representation}.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/flickr30k-document-specification.png}
  \caption[TODO]{TODO: rappresentare i tre file e collegare gli ID con i colori evidenziano l'esempio, le query e le noun phrase per query. Aggiornare immagine con i nuovi fix}
  \label{fig:flickr30k-technical-data-representation}
\end{figure}

Those data ara available online in two parts: images and can be
downloaded by compiling a registration form,
\footnote{\href{http://shannon.cs.illinois.edu/DenotationGraph/}{http://shannon.cs.illinois.edu/DenotationGraph/}}
while annotations, instead, do not requires a sign up: data can be
downloaded from B. A. Plummer repository on GitHub.
\footnote{\href{https://github.com/BryanPlummer/flickr30k\_entities}{https://github.com/BryanPlummer/flickr30k\_entities}}
Within the repository, authors have made available predefined
training, validation and test split that we use in our work, following
current literature.

\subsection{ReferIt}

ReferIt \todo{CITE: referit} is another popular dataset. The reason
that led to developing this dataset is to study how people refer to
objects in complex photographs of a real-world cluttered scenese.
Thus, the focus is on referring expression grounding task. However,
due to the heterogeneity of data, it can be employed also in phrase
grounding.

ReferIt contains very challenging examples from phrase groudning point
of view, because the dataset is collected entirely in an unsupervised
context by crowd throught a game. Examples vary from ``a man'' and
``the person in red'' to ``buildings'' or more degenrative ones like
``i don't think that was trash, lol. anyway, wall just above heads''.
In ReferIt many phrases uses spatial relations among objects, such as
``building on right behind guys'' or ``window top 2nd left''.

Referring expressions are collected through \textit{ReferItGame},
which is a two-player online game. The game is the key point of the
whole dataset, because it allow to collect large-scale dataset
containing natural language expressions referring to objects in
photographs of real world scenes in an inexpansive way.

The game is a simple two player game where players alternate between
generating expressions referring to objects in images of natural
scenes, and clicking on the locations of described objects.
Fig.~\ref{fig:referitgame-example}, from \todo{CITE: referit paper}
shows an example of the game. The game play is straightforward: an
image with an object outlined in red and a text box is show to Player
1. Player 2 instead receives, the same image and the referring
expression written by Player 1 and must click on the location of the
described object (note, Player 2 does not see the object
segmentation). If Player 2 clicks on the correct object, then both
players receive game points and the Player 1 and Player 2 roles swap
for the next image. If Player 2 does not click on the correct object
then no points are received and the players remain in their current
roles. When there are no players available for a player vs player
match, then a canned match is started where the missing player is
replaced by a CPU palyer. In this way, by design the referring
expression are provided and also verificated. 

\begin{figure}
  \includegraphics[width=\textwidth]{figures/referitgame-example.png}
  \caption[ReferIt Game Example.]{An example game from \todo{CITE:
  referit}. Player 1 \textit{(left)} sees an image with an object
  outlined in red (the man) and provides a referring expression for
  the object (``man in red shirt on horse''). Player 2
  \textit{(right)} sees the image and the expression from Player 1 and
  must localize the correct object by clicking on it (click indicated
  by the red square). Elapsed time and current scores are also
  provided.}
  \label{fig:referitgame-example}
\end{figure}

\subsubsection{Data Representation}
\label{subsec:referit-data-representation}

All ReferIt data is available online and instructions are provided at
Licheng Yu's GitHub
repository.\footnote{\href{https://github.com/lichengunc/refer}{https://github.com/lichengunc/refer}}
Images are stored in JPG files, while annotations are described
throught two structured file. One is a JSON file containing the
bounding box annotations along with a list of images and their $id$,
the other is a PICKLE
file\footnote{\href{https://docs.python.org/3/library/pickle.html}{https://docs.python.org/3/library/pickle.html}}
containing a serialized Python object that represents sentence
annotations. The Fig.~\ref{fig:referit-technical-data-representation} depict
described structure.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/referit-document-specification.png}
  \caption[TODO]{TODO: desc}
  \label{fig:referit-technical-data-representation}
\end{figure}

\section{Data preprocessing}

It is well known that data preparation and filtering steps take
considerable amount of processing time in machine learning problems
\cite{kotsiantis2006data}. Most of the time, data we collect or borrow
are biased, flowed, redundant, maybe irrelevant and sometimes
corrupted or missing. For the development of an accurate and general
model, the representation and quality of the instance data must be
maximized by working on task like cleaning, normalization,
transformation, feature extraction and selection. Fortunately, we do
not work with row data: due to the large adoption of datasets
described in Sec.~\ref{sec:datasets}, provided data are already
preprocessed in a grossly way.

In the next section we describe our preprocessing pipeline, focusing
on data representation which can be quite confusing due to the
etherogeneity of representations. In particular,
Sec.~\ref{subsec:data-preparation} describes transformation and
augmentations applied to data, while Sec.~\ref{subsec:data-interface}
descibes the interface we used to represent heterogeneous data.

\subsection{Data Preparation}
\label{subsec:data-preparation}

Given data as specified in
Sec.~\ref{subsec:flickr30k-data-representation} and
\ref{subsec:referit-data-representation} for, respectively, Flickr30K
and ReferIt, we create a single and unified data interface which
exposes features homogeneously and enable the model to work with
different datasets transparently. Then, we augment data with further
information coming from a pretrained object detector.

In the first place, we apply the object detectior mentioned in
Sec~\todo{REF: object detector} to extract features. The proposal
extractor generates a list of bounding box and the object detector
computes $2048$ features per bounding along with two list of
probabilities, repspectively for classes and attributes. For each
bounding box $b$, the object detector predicts a class $c = cls(b)$
such that it semantically represents the content of the box, if it
exists. When the object detector is not able to predict a class with
high confidence, the a default class is predicted, namely
``background''. Also, employed object detector is able to predict
attributes for a given region. In contrast to classes, each bounding
box can be represented by more than one attribute: attributes can be
considered present if their probability is above a predefined
treshold.\footnote{Unfortunately, the implementation of attributes
prediction for the object detector in \todo{CITE: Bottom-Up and
Top-Down Attention for Image Captioning} is erroneous. As they state
in Sec. $6.1$, the probability for each attribute should be a number
between $0$ and $1$ and an attribute is present when its probability
is over $0.2$. Instead, the current implementation returns a softmax
over the labels (our attributes) + 1, the ``background'' attribute.
Probably this error is due to careless copy-pase from class prediction
branch.}

Secondly, in order to fix the coverage problem described in
Sec.~\todo{REF box coverage} we process ground truth bounding boxes
annotated in the dataset. For each ground truth bounding box
$b^{gt}_i$, we have to find the bounding box generated by proposal
extractor $b'$ such that the intersection over union (more details at
\todo{REF: iou}) between the two is maximized: $b' = \arg \max_{b}
\iou(b^{gt}, b)$. Moreover, following all works in literature, if a
noun phrase corresponds to multiple ground truth bounding boxes, we
merge the boxes and use their union region as its ground truth. On the
contrary, if a noun phrase has no associated bounding box, we remove
it from the dataset.

We use the standard split for training, validation, and test set as
defined in  \todo{CITE: Flickr30K Entities}, consisting of 30K, 1K,
and 1K images, respectively.\todo{REMOVE: move or delete}

\subsubsection{Data Interface}
\label{subsec:data-interface}

The data interface is designed to be as flexible and complete as
possible, allowing the adaptation of other datasets with low effort.
In particular our interface includes the follwoing.

\begin{itemize}
  \item An postive integer used as identifier for the example.
  \item A sentence, i.e, the caption of the image.
  \item A list of phrases, i.e., noun phrases in sentence along with
  its counter.
  \item A list of coordinates, representing, for each phrase, the
  ground truth bounding box.
  \item The width and the height of the image.
  \item A list of bounding box coordinates along with the counter of
  the list.
  \item A list of features per bounding box.
  \item A list of class probabilities, i.e., for each class $c$, the
  probabilitity that the bounding box $b$ is of class $c$, $\Pr(\cls(b)
  = c)$, for each bounding box $b$.
  \item A list of attribute probabilities, i.e., for each attribute
  $a$, the probabilitity that the bounding box $b$ is has the
  attribute $a$, $\Pr(\attr(b) = a)$, for each bounding box $b$.
  \item A list of features per bounding box.
\end{itemize}

The Fig.~\ref{lst:data-interface-dict} shows a real word example
exposing data throught the unified interface.

\begin{lstlisting}[style=simplepython,caption=TODO,label={lst:data-interface-dict}]
{
    id: '3359636318', 
    sentence: 'Two people are talking outside of the video game shop next door to the mobile phone store .', 
    phrases: ['Two people', 'the video game shop', 'the mobile phone store'], 
    n_phrases: 3, 
    phrases_2_crd: [[46, 165, 207, 333], [0, 54, 168, 307], [191, 0, 498, 230]], 
    image_w: 500, 
    image_h: 334, 
    image_d: 3, 
    pred_n_boxes: 100, 
    pred_boxes: [[0.0, 280.0, 278.4, 333.4], [395.5, 232.3, 453.2, 333.1], ...], 
    pred_cls_prob: [[0.09, 0.01, ...], [0.27, 0.02, ...], ...],
    pred_boxes_features: [[0.0, 2.0, ...], [2.57, 1.17, ...], ...]
}
\end{lstlisting}


TODO: aggiungere descrizione "formale", elencando le chiavi del dizionario del batch




TODO: discutere operations on single element e varie trasformazioni + il padding e la creazione del batch in un'altra sezione! (pi√π collegata a impl. techine)

\section{Data Analysis}
\label{sec:data-analysis}

Data analysis is another crucial task of data processing pipeline and
it is not only important to process and manipulate data, but also to
analyze the relationships and correlations between data sets and to
identify patterns and trends for results interpretation. \todo{CITE:
Understanding Data Analysis Step-By-Step} Data analysis can be
decisive also in the phase of model design, because it can outline
problems or instrisic traits that belongs to data and must be taken
into account.

Within the folllwing sections we outline our shallow data analysis,
performed on-demand while designing the model in order to develop
winning strategies.

\subsection{Number of Bounding Box per Image}
\label{subsec:num-of-proposals}

Each image in the dataset is different and vary wrt number of objects
in it, type of scene, dimensions and so on. Thus, by nature of images,
the object detector can extract a variable number of bounding boxex
and usually, only a variable subset of these are relevant. However,
for implementation purposes we need to fix a number $k$ of proposal
per example. Then, how to chose $k$? 

Following literature we initially fixed the upper-threshold to $k =
100$ proposals, and then we performed an analysis on both dataset
aimed to detect the distribution of proposals per image. We discovered
that the object detector can detect maximum number of proposal for
almost every example except to some degenerative images.

Such metric allowed us to conclude that even if some boxes are
artifacts (generated with $0$-coords only to fill the vector), they
are pretty rare: XX in Flickr30k and YY in ReferIt \todo{ADD: numbers}
and we can keep $k = 100$ to reach maximum objects coverage.

\subsection{Upperbound Accuracy}

Model performance depends on the degree of overlap between predicted
bounding box and ground truth bounding box (see
Sec.~\ref{sec:evaluation-metric}). However, our model deals with
proposal given by object detector and it has no power in moving them
around. This means that, without the ability to tune proposal
coordinates, the model obtains good performace only if a bounding box
that overlap the ground truth by a certain value exists. So, do the
object detector's proposals cover the ground truth?

We analyzed the data and we discovered that, with $k = 100$ bounding
box the coverage is good enough (nearly $93$-$91$\%), however, by
decreasing $k$ upperbound accuracy drops by a large margin (nearly
$54$-$51$\%) . Results are shown in Tab.~\ref{tab:bb-coverage}.

\begin{table}
  \centering
  \begin{tabular}{c|cc}
     & \multicolumn{2}{c}{Upperbound Accuracy (\%)} \\
    $k$ & Flickr30k & ReferIt \\\hline 
    $100$ & $93.4$ & $91.4$ \\ 
     $90$ & $93.1$ & $90.9$ \\  
     $80$ & $92.7$ & $90.3$ \\
     $70$ & $92.0$ & $89.5$ \\
     $60$ & $91.2$ & $88.5$ \\
     $50$ & $89.8$ & $86.8$ \\
     $40$ & $87.5$ & $84.2$ \\
     $30$ & $83.3$ & $79.7$ \\
     $20$ & $74.5$ & $70.5$ \\
     $10$ & $53.9$ & $51.0$ \\\hline
  \end{tabular}
  \caption[TODO]{TODO}
  \label{tab:bb-coverage}
\end{table}

\subsection{Upperbound Accuracy Removing Background Class}

As stated in Sec.~\todo{REF: section} we remove proposal where
predicted class is ``background''. ``Background'' is a special class
and represents the absence of a classification. In many supervised
works the removal of such potential candidate proposals may impact on
the performance, because every proposal should potentially be the one
that maximize the overlapping with the ground truth and a supervised
model can learn to identify it. Instead, under weakly supervised
settings, this kind of proposals are a source of noise. How can the
model discriminate a positive or negative example based on the
similarity of the phrase with the proposal class label whether the
label is ``background''?

To remove this kind of boxes, as mentioned above, may reduce the
maximum achievable accuracy. We analyzed this phenomenom and we
surprisingly noticed that even removing on average one proposal over
two, the upperbound accuracy remains comparable to the version without
background removal. In particular, we obtained a relative loss of
$4.1$ and $6.6$\% on average and an absolute loss of $3.7$ and $5.8$\%
on average, respectively for Flickr30k and ReferIt, wrt to upperbound
accuracy without background removal. Analytics details are reported in
Tab.~\ref{tab:bb-coverage-no-background}.

Such results allow us to apply the background removal with translates
in noise removal without any relevant drop in performace.

\begin{table}
    \centering
    \begin{tabular}{c|cc|cc}
       & \multicolumn{2}{c|}{Flickr30k} & \multicolumn{2}{c}{ReferIt} \\\hline
      $k$ & \multicolumn{1}{p{2.2cm}|}{\centering Upperbound\\ Accuracy (\%)} & \multicolumn{1}{p{2.2cm}|}{\centering Average Box\\ Removed (\%)} & \multicolumn{1}{p{2.2cm}|}{\centering Upperbound\\ Accuracy (\%)} & \multicolumn{1}{p{2.2cm}}{\centering Average Box\\ Removed (\%)}   \\\hline 
      $100$ & $88.3$ & $47.6$ & $83.0$ & $51.1$ \\
       $90$ & $87.9$ & $46.6$ & $82.6$ & $49.9$ \\
       $80$ & $87.5$ & $44.8$ & $82.2$ & $48.4$ \\
       $70$ & $86.9$ & $42.7$ & $81.6$ & $46.4$ \\
       $60$ & $86.3$ & $40.0$ & $80.9$ & $43.7$ \\
       $50$ & $85.3$ & $36.3$ & $80.1$ & $39.9$ \\
       $40$ & $83.8$ & $30.8$ & $78.8$ & $34.5$ \\
       $30$ & $80.8$ & $22.4$ & $76.0$ & $26.4$ \\
       $20$ & $73.6$ & $10.8$ & $68.9$ & $15.0$ \\
       $10$ & $53.9$ &  $1.2$ & $50.8$ &  $3.8$ \\\hline
    \end{tabular}
  \caption[TODO]{TODO}
  \label{tab:bb-coverage-no-background}
\end{table}

\subsection{Distribution of Class Labels on Proposals}

For a good comprehension of object detector information, we study the
distribution of number of examples per label. We perfomed the analysis
on both dataset but for presentation purposes, here we show
qualitative data from Flickr30k; ReferIt shows comparable insights.
The distribution, shown in
Fig.~\ref{fig:flickr30k-label-distribution}, is a long-tail
distribution. This means that there are few labels with a very large
population, while many other labels count little number of examples.
We truncate the plot to top-100 labels with larger population and the
phenomenon is clearly visible. In
Tab.~\ref{tab:flickr30k-label-distribution-nobg-top10} we report the
top-10 labels per number of examples. Except to the evident gender gap
effect in data, we show that there is a great discrepancy in terms of
number of examples for labels like \textit{man}, \textit{woman},
\textit{people} and the others. Also, the special \textit{background}
class is very popular: we show that removing it preserves order
between most popular labels.

\begin{figure}
  \includegraphics[width=0.8\textwidth]{figures/number-of-examples-per-label-over-top-100-most-popular-labels.png}
  \caption[Number of examples per label over top-100 most popular labels on Flicker30k]{Distribution of examples ($y$ axis, log scaled) per label over top-100 most popular labeles ($x$ axis) on Flicker30k.}
  \label{tab:flickr30k-label-distribution-top10}
\end{figure}

\begin{table}
  \begin{subtable}{.5\linewidth}
    \centering
    \begin{tabular}{lc}
      Class                & N       \\\hline
      man                  & $71120$ \\
      \textit{background}  & $58014$ \\
      woman                & $34421$ \\
      people               & $29121$ \\
      shirt                & $19129$ \\
      boy                  & $11182$ \\
      dog                  & $9427$  \\
      girl                 & $7292$  \\
      water                & $7118$  \\
      wall                 & $6358$  \\
    \end{tabular}
  \end{subtable}%
  \begin{subtable}{.5\linewidth}
    \centering
    \begin{tabular}{lc}
      Class  & N       \\\hline
      man    & $78250$ \\
      woman  & $37751$ \\
      people & $33149$ \\
      shirt  & $22585$ \\
      boy    & $12019$ \\
      dog    & $9989$  \\
      wall   & $7905$  \\
      girl   & $7886$  \\
      water  & $7674$  \\
      ground & $7547$  \\
    \end{tabular}
  \end{subtable} 
  \caption{Distribution of examples over top-10 labels with larger population with (Left) and without (Right) \textit{backgorund} class.}
  \label{tab:flickr30k-label-data-top10}
\end{table}

\section{Evaluation Metric}
\label{sec:evaluation-metric}

Aligned with the works in literature, we consider the standard
accuracy metric. Given a noun phrase, it considers a bounding box
prediction to be correct if and only if the intersection over union
(Sec.~\todo{REF: iou}) value between the predicted bounding box and
the ground truth bounding box is at least 0.5. Formally, accuracy between $B* = \{ b* | b = model(\bm{I}, S) \}$ the set of predicted bounding boxes and $B^{gt}$ the set of ground truth bounidng boxes, can be defined as
\[
  acc(B^*, B^{gt}) = \frac{\sum_{i = 1}^{n} \One (\iou (b_i^*, b_i^{gt}) \geq 0.5)}{n}.
\]

Moreover, we also calculate the pointing game accuracy in order to
evaluate model's performance in localizing the correct bounding box in
the space, even if it is considered very optimistic with respect to
the standard accuracy. Poiting game accuracy consider an example to be
positive whether the center of predicted bounding box lies wherever
inside the ground truth box. More formally,
\[
  acc_{point}(B^*, B^{gt}) = \frac{\sum_{i = 1}^{n} \One (c_i^* \in  b_i^{gt})}{n},
\]
where $c^* = (x + \frac{w}{2}, y + \frac{h}{2})$ with $x, y$ the
bounidng box top-left point and $w, h$ its width and height. Note that
$c^* \in b^{gt}$ is an abuse of notation, we mean true when $c^*$ is
inside the box boundaries.