%!TEX root = ../dissertation.tex

\begin{savequote}[75mm]
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname}
\end{savequote}

\chapter{Background}

TODO

* object detector (theory)
* rnn + lstm
* textual embedding
* vocabulary (move to impl. details)

\section{Recurrent Neural Network}

A recurrent neural network (RNN) is a special type of an artificial
neural network (ANN) adapted to work for time series data or data that
involves sequences. Ordinary feed forward neural networks are only
meant for data points independent of each other while RNN introduces
the concept of ``memory'' that helps them store the states or
information of previous inputs to generate the next output of the
sequence by sharing weights.

An ANN with recursion can be easily modeled by the Eq.\ref{eq:rnn}:

\begin{equation}
  \label{eq:rnn}
  \bm{h}^{(t)} = f ( \bm{h}^{(t - 1)}, \bm{x}^{(t)} ; \bm{\theta} ),
\end{equation}

where $\bm{x}^{(1)}, \ldots, \bm{x}^{(\tau)}$ is the input sequence,
$f$ is the function that maps hidden units at time step $t - 1$ to
time step $t$ and $\bm{\theta}$ the parameters of $f$. The equation is
recurrent because the deﬁnition of $\bm{h}$ at time $t$ refers back to
the same deﬁnition at time $t - 1$. The process is visually depicted
in Fig.\ref{fig:rnn-with-unfold} \emph{(Left)} where the black square
indicates a delay of a single time step. At this point the problem is:
how do we propagate our input throught the RNN? Here comes in the idea
of unfolding. To unfold an RNN basically means to unroll the cyclic
circuit in a graph by explicitly applying the function $f$ on
$\bm{x}^{(t)}$, the input at time $t$, and $\bm{h}^{(t-1)}$, the
hidden state at time $t - 1$. RNN unfolding is visually explained in
Fig.\ref{fig:rnn-with-unfold} \emph{(Right)}.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{figures/rnn-with-unfold.png}
  \caption[TODO]{TODO: desc}
  \label{fig:rnn-with-unfold}
\end{figure}

Tipycal RNNs add extra architectural features such as output layers
that read information out of the state $\bm{h}$ to make predictions.

When the recurrent network is trained to perform a task that requires
predicting the future from the past, the network typically learns to
use $\bm{h}^{(t)}$ as a kind of lossy summary of the task-relevant
aspects of the past sequence of inputs up to t. This summary is in
general necessarily lossy, since it maps an arbitrary length sequence
$( \bm{x}^{(t)}, \bm{x}^{(t-1)}, \bm{x}^{(t-2)} , \ldots ,
\bm{x}^{(2)}, \bm{x}^{(1)} )$ to a ﬁxed length vector $\bm{h}^{(t)}$ .
Depending on the training criterion, this summary might selectively
keep some aspects of the past sequence with more precision than other
aspects. For example, if the RNN is used in statistical language
modeling, typically to predict the next word given previous words, it
may not be necessary to store all of the information in the input
sequence up to time $t$, but rather only enough information to predict
the rest of the sentence.

Recurrent neural networks can be built in many diﬀerent ways with
varying architecture. In literature emerged some successful RNNs
architecture with a well defined and non-casual structure, such us
Bidirection RNN (Bi-RNN), Long Short-Term Memory (LSTM) and Gated
Recurrent Unit (GRU). Each RNN architecture has some pro and cons and
can be applied successfully to specific tasks.

\subsection{Bidirectional RNN}

As the name suggests, bidirectional RNNs combine an RNN that moves
forward through time beginning from the start of the sequence with
another RNN that moves backward through time beginning from the end of
the sequence. Those networks have been applied successfully when the
output we want to predict depetnds on the whole input sequence. For
example, in speech recognition, the correct interpretation of the
current sound as a phoneme may depend on the next few phonemes because
of co-articulation and potentially may even depend on the next few
words because of the linguistic dependencies between nearby words: if
there are two interpretations of the current word that are both
acoustically plausible, we may have to look far into the future (and
the past) to disambiguate them. This is also true of handwriting
recognition and many other sequence-to-sequence learning tasks such us
bioinformatics.

The Bi-RNNs convoy an intrisic advantage and drawback at the same
time: it requires the full sequence, so it cannot work with real-time
input sequences, but for the same reason it can better infer the
output from the past and future kwown context.

\subsection{Gated RNN (LSTM and GRU)}

Being able to work with long term dependencies is crucial for many
applications: think, for example, to a dialogue system which should
remember previous parts of the speech on order to keep on with the
dialogue.

Unfortunately, RNNs and long term dependencies yield a mathematical
challenge. The basic problem is that gradients propagated over many
stages tend to either vanish (most of the time) or explode (rarely,
but with much damage to the optimization). Moreover, I. Goodfellow
\etal{} in \todo{CITE: deeplearningbook}, states also that:

\begin{quote}
  Even if we assume that the parameters are such that the recurrent
  network is stable (can store memories, with gradients not
  exploding), the diﬃculty with long-term dependencies arises from the
  exponentially smaller weights given to long-term interactions
  (involving the multiplication of many Jacobians) compared to
  short-term ones.
\end{quote}

One way to deal with long-term dependencies is to design a model that
operates at multiple time scales, so that some parts of the model
operate at ﬁne-grained time scales and can handle small details, while
other parts operate at coarse time scales and transfer information
from the distant past to the present more eﬃciently. Various
strategies for building both ﬁne and coarse time scales are possible.
These include the addition of skip connections across time, ``leaky
units'' that integrate signals with diﬀerent time constants, and the
removal of some of the connections used to model ﬁne-grained time
scales.

Like leaky units, gated RNNs are based on the idea of creating paths
through time that have derivatives that neither vanish nor explode.
Leaky units did this with connection weights that were either manually
chosen constants or were parameters. Gated RNNs generalize this to
connection weights that may change at each time step.

Leaky units allow the network to accumulate information (such as
evidence for a particular feature or category) over a long duration.
However, once that information has been used, it might be useful for
the neural network to forget the old state. For example, if a sequence
is made of sub-sequences and we want a leaky unit to accumulate
evidence inside each sub-subsequence, we need a mechanism to forget
the old state by setting it to zero. Instead of manually deciding when
to clear the state, we want the neural network to learn to decide when
to do it.

LSTM basically integrates the idea of self-loops to produce paths
where the gradient can ﬂow for long durations with the addition to
control weight on this self-loop conditioned on the context, rather
than ﬁxed. By making the weight of this self-loop gated (controlled by
another hidden unit), the time scale of integration can be changed
dynamically. LSTM recurrent networks have ``LSTM cells''that have an
internal recurrence (a self-loop), in addition to the outer recurrence
of the RNN. Fig.~\ref{fig:lstm} illustrate the LSTM block diagram. In
an LSTM cell, the self-loop controls the flow of information from
input to output through three key components: a forget gate, an
external input gate and an output gate.

\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{figures/lstm.png}
  \caption[TODO]{TODO: desc}
  \label{fig:lstm}
\end{figure}

Along with LSTM architecture, a simpler version was recently proposed
with the aim to simplify cells and speed-up computations: the gated
recurrent units (GRU). The behaviour wrt LSTM is very similar, the
only difference is that a single gating unit simultaneously controls
the forgetting factor and the decision to update the state unit.

\section{Object detection and recognition}

Object detection (OD) and object recognition (OR) systems are crucial
in a wide variety of everyday tasks such us face detection, image and
video databases information retrieval, surveillance applications,
driver assistance, self drive, robotics, automation and, specially in
vision tasks, it is a foundamental building block used to extract
information from images.

The primary essence of those systems can be broken down into two
parts: to locate objects in a scene such us by drawing a bounding box
around the object (object detection) and later to classify the objects
based on the classes it was trained on (obejct recognition). OD and OR
are Often used together and for this reason the name of two tasks is
usually used interchangeably.

We can group the state-of-the-art detection systems in two main
approaches: one-stage methods (YOLO - You Only Look Once \todo{CITE:
yolo}, SSD - Single Shot Detection \todo{CITE: ssd}) and two-stage
approaches (R-CNN, Fast R-CNN, Faster R-CNN \todo{CITE: RCNN}).

\newthought{One stage methods}

\begin{enumerate}[label=(\alph*)]
  \item YOLO -- \emph{You Only Look Once} is a state-of-the-art,
  real-time object detection system which offers extreme levels of
  speed and accuracy. YOLO reframes object detection as a regression
  problem to spatially separated bounding boxes and associated class
  probabilities. A single neural network predicts bounding boxes and
  class probabilities directly from full images in one evaluation.
  Since the whole detection pipeline is a single network, it can be
  optimized end-to-end directly on detection performance. This
  architecture offers some advantages wrt to other methods: first,
  YOLO is extremely fast and second, it reasons globally about the
  image when making prediction implicitly encoding contextual
  information about classes as well as their appearance. Last but not
  least, YOLO is more likely to learn generalizable representations of
  objects because of its global reasoning on image.
  \item SSD -- \emph{Single Shot Detection} another state-of-the-art
  method for detecting objects in images using a single deep neural
  network. SSD discretizes the output space of bounding boxes into a
  set of default boxes over different aspect ratios and scales per
  feature map location and, at prediction time, the network generates
  scores for the presence of each object category in each default box
  and produces adjustments to the box to better match the object
  shape. Additionally, the network combines predictions from multiple
  feature maps with diﬀerent resolutions to naturally handle objects
  of various sizes. Also, SSD offers high speed performance due its
  single network and for this reason it is easy to train and
  straightforward to integrate into systems that require a detection
  component. In terms of accuracy, experimental results conﬁrm that
  SSD has competitive accuracy also in low resolution compared to
  methods that utilize an additional object proposal step.
\end{enumerate}

\newthought{Two stage methods}

\begin{enumerate}[label=(\alph*)]
  \item R-CNN -- \emph{Regions with CNN} is the first performant
  object detection system ever built. It is composed by a two-stage
  architecture: in the first step the Selective Search algorithm
  generates around 2000 category-independent region proposals for the
  input image, in the last step instead it extracts a fixed-length
  feature vector from each proposal using a CNN (hence the name
  R-CNN), and then classifies each region with category-specific
  linear SVMs. The method shows interesting results and can be applied
  also with scarce data availability: the system can be pretrained and
  the fine-tuned. In temrs of computation time, R-CNN performs some
  expensive operations required for greedy non-maximum suppression,
  amogn others, but on relatively small inputs.
  \item Fast R-CNN -- The Fast R-CNN model was built to counter a few
  drawbacks of the previous R-CNN model. In this approach, similar to
  the previous, Selective Search is used to generate region proposals
  but the input image is fed to a CNN and a convolutional feature map
  is generated from it which is then used to identify the regions and
  combine them into larger squares by using a RoI pooling layer. A
  softmax layer is finally used to predict the class of the proposed
  region.
  \item Unlike R-CNN and Fast R-CNN, Faster R-CNN does not use
  Selective Search which is a slow process. Instead, it allows the
  network to learn the region proposals throught a separate network,
  able to predict the region proposals. The predicted proposals are
  then pooled into larger squares using the RoI pooling layer which is
  then finally used to classify the image.
\end{enumerate}

\todo{ADD: quale abbiamo scelto? attenzione a quanto già detto in introduzione (potrebbe essere spostato qui...)}

\section{Word Embeddings}

Being able to model and represent natural language features is a
relevant machine learning task that belongs to the natural language
processing field and has many applications, such us information
retrieval (Manning et al., 2008), document classification (Sebastiani,
2002), question answering (Tellex et al., 2003), named entity
recognition (Turian et al., 2010), and parsing (Socher et al., 2013).
\todo{CITE: recupeare i paper dal paper GloVe} 

Instead of treating words as atomic units where there is no notion of
similarity between words, as these are represented as indices in a
vocabulary, natural language information can be represented as
real-valued feature vectors throught a semantic vector space, where
representation of words are learned for a predefined fixed sized
vocabulary from a corpus of text and the intrinsic quality of such a
set of word is evaluated by the distance or angle between pairs of
word vectors representations or by their various dimensions of
difference\todo{CITE:  Mikolov et al. (2013c)}. In light of this we
can formulate the definition for word embedding: 

\begin{quote}
    \textit{A word embedding is a learned representation for text
    where words that have the same meaning have similar
    representation.}
\end{quote}

With the years, literature explored different approaches for learning
good words representation: either joint with the neural network model
on some task, such as document classification, or using document
statistics throught unsupervised settings.

\subsection{Indexing by Latent Semantic Analysis}

A first approach for learning representative word embeddings was first
introduced by S. Deerwester \etal{} in \todo{CITE: Deer- wester et
al., 1990}, exploiting global matrix factorization by means of latent
semantic analyis (LSA). The proposed approach tries to overcome the
deficiencies of term-matching retrieval by treating the unreliability
of observed term-document association data as a statistical problem.
Indeed, they take advantage of a particular version of LSA that uses
singular-value decomposition. First, a large matrix of term-document
association data is crteated and then a semantic space wherein terms
and documents that are closely associated are placed near one another
is constructed. Singular-value decomposition allows the arrangement of
the space to reflect the major associative patterns in the data, and
ignore the smaller, less important influences.

\subsection{Word2Vec}

Word2Vec, introduced by T. Mikolov in \todo{CITE: w2v-1} and then
updated and revisioned in \todo{CITE: w2v-2, w2v-3}, is a statistical
method for efficiently learning a standalone word embedding from a
text corpus. One of the goal of Word2Vec is to learn distributed
representations of words by neural networks, in particular, they
introduced two different learning models that can be used as part of
the word2vec approach to learn the word embedding: Continuous
ag-of-Words (CBOW) and Continuous Skip-Gram Model. The CBOW model
learns the embedding by predicting the current word based on its
context. The continuous skip-gram model learns by predicting the
surrounding words given a current word. Both models, shown in
Fig.~\ref{fig:word2vec-learning-models} are focused on learning about
words given their local usage context, where the context is defined by
a window of neighboring words. This window is a configurable parameter
of the model. The quality of these representations is measured in a
word similarity task.

\begin{figure}
  \includegraphics[width=\textwidth]{figures/word2vec-training-models.png}
  \caption[TODO]{TODO: desc}
  \label{fig:word2vec-learning-models}
\end{figure}

The key benefit of the approach is that high-quality word embeddings
can be learned efficiently (low space and time complexity), allowing
larger embeddings to be learned (more dimensions) from much larger
corpora of text (billions of words). Also, Word2Vec perform
significantly better than LSA for preserving linear regularities among
words and LDA moreover becomes computationally very expensive on
large data sets.

\subsection{GloVe}

GloVe is an extension to the Word2Vec method for efficiently learning
word vectors, developed by Pennington \etal{} in \todo{CITE: glove},
that wants to to marry both the global statistics of matrix
factorization techniques like LSA with the local context-based
learning in Word2Vec. Rather than using a window to define local
context, GloVe constructs an explicit word-context or word
co-occurrence matrix using statistics across the whole text corpus.
The result is a learning model that may result in generally better
word embeddings.

\subsection{Other methods}

\newthought{BERT} -- Bidirectional Encoder Representations from Transformers is a relatively recent language representation model, introduced by J. Devlin \etal{} in \todo{CITE: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},  designed to pretrain deep bidirectional representations from
unlabeled text by jointly conditioning on both left and right context in all layer. BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training.

\todo{ADD OR REMOVE: Both families suffer significant drawbacks. While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure. Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts.}

\section{Similarity Measures}

\subsection{Intersection over Union (IoU)}

Given a pair of bounding box coordinates (b i , b j ), the
Intersection over Union (IoU), also known as Jaccard index, is an evaluation metric used mainly in object detection
tasks, which aims to evaluate how much the two bounding
box refer to the same content in the image. Specifically, it
is defined as:
\[
  \iou(\bbox_i, \bbox_i) = \frac{| \bbox_i \inters \bbox_j |}{| \bbox_i \union \bbox_j |}
\]
where $| \bbox_i \inters \bbox_j |$ is the area of the box obtained by
the intersection of boxes $\bbox_i$ and $\bbox_j$ , while $| \bbox_i
\union \bbox_j |$ is the area of the box obtained by the union of
boxes $\bbox_i$ and $\bbox_j$. It is invariant to the bounding boxes
sizes, and it returns values that are strictly contained in the
interval $[0, 1] \in \Rset$, where $1$ means that the two bounding
boxes refer to the same image area, while a score of $0$ means that
the two bounding boxes do not overlap at all. The fact that two
bounding boxes that do not overlap have IoU score equal to $0$, is the
major issue of this metric: the zero value does not represent how much
the two bounding boxes are far from each other. For this reason, in
its standard definition, the intersection over union is mainly used as
an evaluation metric rather than as a component of a loss function for
learning.

\subsection{Cosine Similarity}

Cosine similarity is a metric used to measure how similar
representations are irrespective of their size (norm). Mathematically,
it measures the cosine of the angle between two vectors projected in a
multi-dimensional space:
\begin{equation}
  s_{\cos}(\bm{a}, \bm{b}) = \frac{\veca \cdot \vecb}{\normtwo\veca \normtwo\vecb},
\end{equation}
where $\veca \cdot \vecb$ is the dot product between two vectors and
$\normtwo\veca$ is the L2-norm of a vector.
