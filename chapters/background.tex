%!TEX root = ../dissertation.tex

\begin{savequote}[75mm]
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname}
\end{savequote}

\chapter{Background}
\label{ch:background}

\section{Visual Grounding}
\label{sec:visual-grounding}

Visual grounding is the general task of locating the components of a
strucutred description in an image. Due to the ambiguous
interpretation of the general problem, it is usually decomposed in two
separated sub-problems, namely Referring Expression Grounding (REG)
and Phrase Grounding (PG).

\newthought{Referring Expression Grounding} is the simple task of
locating the subject referred by an expression in given image.

More formally, given in input an image $\bm{I}$ and a sentence
$\mathrm{S}$, REG consists in learning a function $\delta$ from the
set $\calS$ of sentences to a set of bounding boxes $B$ defined on
$\bm{I}$, i.e., $\delta: \calI \times \calS \rightarrow 2^{\calS
\times \calB}$, where $\calI$ is the domain of images, $\calS$ is the
domain of sentences, $\calB$ is the domain of bounding boxes which can
be defined on $\calI$, and $2^{\calS \times \calB}$ is the power set
of the cartesian product between $\calS$ and $\calB$.

\newthought{Phrase Grounding}, instead, is the task that studies the
mapping from noun phrases to regions of an image. In order to solve
this task, first, it is necessary to recognize all the objects in the
image and the components in the text, while after, the model needs to
find the correct alignment among the nouns and the objects.

More formally, given in input an image $\bm{I}$ and a sentence
$\mathrm{S}$, phrase grounding consists in learning a mapping $\gamma$
from the set $\calQ$ of noun phrases to a set of bounding box $\calB$
defined on $\bm{I}$, i.e., $\gamma : \calI \times \calS \rightarrow
2^{\calQ \times \calB}$. So, given an image $\bm{I}$ containing $e$
objects identified via the set of bounding boxes $B_\calI = \{ b_i
\}^e_{i=1}$, where $b_i \in \Rset^4$ is the vector of coordinates
identifying a bounding box in $\bm{I}$, and a sentence $\mathrm{S}$
containing $m$ noun phrases gathered in the set $\calQ_S = \{ q_j
\}^m_{j=1}$, where $q_j \in \Nset^2$ is a vector containing as
coordinates the initial and final character positions in the sentence
$\mathrm{S}$, $\gamma(\bm{I}, \mathrm{S})$ returns a set of couples
$\{ (\bm{q}, \bm{b}) \mid \bm{q} \in \calQ_{\mathrm{S}}, \bm{b} \in
B_{\bm{I}} \}$ where each couple $(\bm{q}, \bm{b})$ associates the
noun phrase $\bm{q}$ to the bounding box $\bm{b}$. Please, notice that
the same noun phrase can be associated to several different bounding
boxes, as well as the same bounding box can be associated to many
different noun phrases. Following the current literature, in this
paper we assume that each noun phrase is associated to one and only
one bounding box. Bounding boxes, however, can identify more objects,
e.g. several persons in the case the noun phrase is ``people''.

\section{Two Stage vs One Stage}
\label{sec:two-stage-vs-one-stage}

Visual grounding is a challenging task which requires the semantic
understanding of the image content and its textual description. In
order to lower the complexity of the whole task, visual grounding is
generally formulated as an object detection task followed by a
classification task in which, given an input image and sentece, the
goal is to return only the detected object(s) that represents the best
semantic match with the sentence. This approach was extensively used
by many researchers in visual grounding field
\cite{rohrbach2016grounding, xiao2017weakly, akbari2019multi, chen2018knowledge, datta2019align2ground, wang2020maf}
due to its simplicity. On the other hand, some recent works started
introducing the one stage approach \cite{yang2019fast,sadhu2019zero}
in which the object detection and the classification problem are
solved joinly, arguing this new architecture may lead to faster and
more accurate models.

\newthought{The two stage approach} is a simple approach to the visual
grounding task which relies on the decomposition of the main task in
two sub-tasks, namely object detection and classification. Given an
input image, the first step generates candidate regions using an
object proposal extractor such us Edge Boxes \cite{zitnick2014edge}
and Selective Search \cite{uijlings2013selective} or by an object
detector, such us Faster R-CNN \cite{girshick2014rich}, Single Shot
multibox Detector (SSD) \cite{liu2016ssd}, or YOLO
\cite{redmon2016you}. The second step consists in ranking this
candidate regions, which hopefully capture the semantic context,
conditioned on a language query about the image. Practically, this
means that the model predicts, for each proposal bounding box, a score
that represents how much the content of the bounidng box is likely to
be referred by the sentence. Sometimes, models predict also new
coordintas for the best predicted proposal, in order to better fit the
visual content accordingly to the sentence semantic information. The
two stage method is outlined in Fig.~\ref{fig:two-stage-approach}.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{figures/two-stage-approach.png}
  \caption[Two stage approach]{
    The two stage approach. The grounding system receives a set of
    proposel extracted by a proposal generation system and, given the
    query, it aligns entities in query with (fixed) set of proposal.
  }
  \label{fig:two-stage-approach}
\end{figure}

Even if the approach seems very promising, it is not exempt from some
problems. As reported in \cite{yang2019fast} ``the propose-and-rank
two stage framework is flawed in at least two major ways''. First of
all, if none of the region candidates of the first stage hits the
ground truth region, the whole framework fails no matter how good the
second ranking stage could perform. Note that a hit is considered
successful if any of the proposals overlap the ground truth by more
than fifty percent in term of intersection over union (IoU). Secondly,
they argue that the process can be optimized by focusing on relevant
bounding boxes which are often a small number (2-5), instead of waste
lot of resources by computing thousands of proposal and then rank them
down to a list.

However, the highlighted problems highly depends on chosen proposal
extractor/object detector. Consider, for example, the problem of
hitting the ground truth with at least one bounding box. Given an
image $\bm{I}$ and $\bm{b}'$ a ground truth bounding box, if we
employee a very simple (yet extremely inefficient) proposal extractor
such us the one who generates $\calP^*_{\bm{I}}$ the set of all
possible bounding boxes in $\bm{I}$, then, by construction we have
that $b' \in \bm{B}$. Hence, the more proposals we consider, the
better we overlap with $\bm{b}'$. However, this solution is bound by
time and memory constraints, but fortunately we do not need to
generate all possible proposals. Indeed, only a small subset of all
solution are required to hit the ground truth bounding box. Thanks to
powerful object detection system we can recognize relevant objects in
image and focus only on those proposals.

Practically, we proved (Sec.~\ref{subsec:num-of-proposals}) that with
$100$ bounding boxes and Faster-R CNN with Bottom-Up Attention
\cite{sharma2020understanding} as object detector, we obtain a
coverage which is greater than 90\% on both two major dataset in the
field of study: Flickr30k and ReferIt.

The second problem instead is partially solvable because the system,
in order to perform its classification task, needs to consider all the
extracted proposal. However, the first stage can be precalculated and
cached for the second stage, drastically reducing the required
resources at training time. At inference time, instead, we need to run
both the object detector and the model without the possibility to
reduce computational time.

\newthought{The one stage approach}, outlined in
Fig.~\ref{fig:one-stage-approach} relies on a single step for
computing the solution. The model receives in input an image and a
textual sentence, and it should learn how to extract and fuse all the
visual and textual information in order to predict the best bounidng
box in output, according to the input sentence. 

The thight coupling provided by this approach may lead to higher
results with lower computational resources \cite{yang2019fast} and can
be used when there is no fixed set of categories \cite{sadhu2019zero},
but it raises some major issues. First of all not all the visual
grounding datasets are suitable for training an object detector, due
to lack of images and/or because they are not densely annotated.
Secondly, the model requires an high number of parameters, and because
of that the training requires significant computing resources
\cite{rigoni2021better}.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{figures/one-stage-approach.png}
  \caption[One stage approach]{
    The one stage approach. The grounding system receives features
    encoded by a custom network with jointly process visual and
    textual features.
  }
  \label{fig:one-stage-approach}
\end{figure}

\section{Fully/Weakly/Self/Un-Supervised Settings}

Machine learning has achieved great success in various tasks,
particularly in supervised learning tasks such as classification and
regression, but it is noteworthy that in many other tasks it is
difficult to get strong supervision information like fully
ground-truth labels due to the high cost of the data-labeling process.
Visual grounding is a striking example of that. 

In visual grounding, large amount of annotations are required. Think
at the example in Fig.~\ref{fig:dog-playing-with-ball} along with the
sentence "A collie plays with a white ball in a field of green grass",
to perform phrase grounding task we need a lot of data: 
\begin{itemize}
  \item The image;
  \item The sentence;
  \item The annotations identifying queries in the sentence, for
  example ``the dog'', ``a ball'' and ``a field of green grass'';
  \item The annotations identifying entities in the image, i.e., the
  bounding box of the dog, of the ball and of the field with green
  grass.
  \item The annotations that links queries and boxes in the image,
  which are $\calO(mk)$ (fortunately, usually lower than that) where
  $m$ is the number of queries and $k$ the number of bounding box.
\end{itemize}
The collection of such amout of data needs a lot of effort and human
work to be carried out. Furthermore, because of the requirement of
human annotators, gathered data can be erroneous and biased. In
Sec.~\ref{sec:datasets} we extensively discuss two experiments (among
the others) that successfully collected such data and are became
State-of-the-Art datasets in phrase grounding.

It's straightforward to see that this approach cannot scale: larger
sets of annotations are exponentially difficult and expansive to
build. For this reason, researches usually redefine the phrase
grounding task under harder settings where less data are involved.
This is the case of weakly-supervised, or even, unsupervised visual
grounding. The Fig.~\ref{fig:full-weak-no-supervision} shows the
differences between the three supervised settings. In the first case,
a key information is available: the link between the query and the
correct box to ground. Under weakly supervised scenario instead, such
annotations are not available: we only know whether a query is linked
to an image, involving all its bounding box but not which is the
correct one. Under the unsupervised settings insteat we do not have
access to ground truth, we have a corpus of text and a set of images
without link between them.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{figures/full-weak-no-supervision.png}
  \caption[Differences between full, weak and no supervision]{
    Differences between full, weak and no supervision
    \cite{wang2019phrase}. Conventional settings require phrase and
    image localization annotations (fully supervised) or phrase and
    image pairs (weakly supervised) at training time. In contrast, the
    non-paired setting does not provide such annotations for training.
  }
  \label{fig:full-weak-no-supervision}
\end{figure}

A simpler way to think at differnt supervision settings for phrase
grounding is by analogy with the ``match the shape'' game shown in
Fig.~\ref{fig:match-the-shape-game}, a game for children that make
them learn to recognize shapes like cube, cylinder, sphere and so on.
The gameplay is very simple: we are provided some pieces and we win
when all pieces fit in the hole on the board. Here, the model is
represented by the kid, and the input that in phrase grounding would
have been an image made by bounding box proposals and some queries are
the board along with holes and wooden pieces to fit on boards. Now,
consider a kid playing with two of such boards, the first has holes
for geometric shapes (cube, cylinder, sphere, etc.), while the second
has holes for tiny wooden animals (cow, cat, dog, fish, etc.). The
goal is to learn learn to fit pieces on boards. He starts without
knowledge, so he just tries. Pieces that fit in the holes will not
fall, so the kid undestand whether a try is correct or not. After some
time of playing he has learnt where to put the cube and where to put a
cow. This is how the kid (or a grounding model) learns in fully
supervised scenario. Consider instead the scenario where the kid plays
with this two board, but he cannot try to put pieces in holes, he can
just ask whether a piece belongs to the board of shapes or animals. In
this much more complex way of playing, we give to kid only a weak
information: we tell him that a piece will fit on a board rather than
on the other, but we do not tell him which is the correct hole. This
is the weak supervision. In unsupervised settings instead the kid
cannot try neither to fit pieces on boards, nor to ask anything. He is
just left to itself.

\begin{figure}[H]
  \centering
  \includegraphics[width=.4\textwidth]{figures/match-shapes-game.jpg}
  \caption[Match the shape game]{
    Match the shape game. A kid game useful to learn to recognize
    shapes. The kid has to fit colored pieces in their respective
    holes.
  }
  \label{fig:match-the-shape-game}
\end{figure}

More abstractly, in fully supervised settings a training example
consists of two parts: a feature vector (or instance) describing the
event/object, and a label indicating the ground truth output.
Projected in phrase grounding this means knowing the map $\gamma :
\calI \times \calS \rightarrow 2^{\calQ \times \calB}$ that links the
ground truth bounding box $\bm{b}_i \in \calB$ with its query
$\bm{q}_j \in \calQ$.

Another form of supervision is the weak supervision. Broadly speaking,
weakly supervised learning is an umbrella term covering a variety of
studies that attempt to construct predictive models by learning with
weak supervision, and typically, three types of weak supervision are
identified \cite{zhou2018brief}.

\begin{itemize}
  \item The first is incomplete supervision (also known as semi
supervision \cite{rohrbach2016grounding}), i.e. only a (usually small)
subset of training data is given with labels while the other data
remain unlabeled. Given an example $(\bm{I}, S, G)$, $G = \{
(\bm{q}^{gt}_j, \bm{b}^{gt}_j) \}^{m'}_{j=1}$ where $m' \leq m$ with
$m$ the number of noun phrases, in incomplete supervision we may lack
some ground truth in $G$.
  \item The second type is inexact supervision (also known as weak
supervision), i.e. only coarse-grained labels are given. In phrase
grounding inexact supervision means we have weak ground truth, that
is, we are given the pair $(\bm{I}, S)$ where we know only the link
between image and sentence but not the link between bounding boxes and
queries.
  \item The third type is inaccurate supervision, i.e. the given
labels are not always ground truth. In other words, some label
information may suffer from errors.
\end{itemize}

Inaccurate supervision is usually mixed with other settings because
the labelling procedure is carried out by human annotators which can
introduct errors.

\section{NLP and Part of Speech Tagging}

Part of Speech (POS) Tagging is a very basic and well known Natural
Language Processing (NLP) problem which consists of assigning to each
word of a text the proper morphosyntactic tag in its context of
appearance \cite{marquez2000machine}. It is very useful for a number
of NLP applications: as a preprocessing step to syntactic parsing,
in information extraction and retrieval (e.g. document classification
in internet searchers), text to speech systems, corpus linguistics,
etc. The base of POS tagging is that many words being ambiguous
regarding their POS, in most cases they can be completely
disambiguated by taking into account an adequate context. Furthermore,
there are even cases in which the ambiguity is non-resolvable using
only morphosyntactic features of the context, and require semantic
and/or pragmatic knowledge. For instance, in the sample sentence
``Yesterday I read the paper'' the word \textit{read} is disambiguated
as a past simple because the word \textit{yesterday} at the beginning
of the sentence. Without context, the POS tagger can tag \textit{read}
also as a present or past participle.

Starting with the pioneer tagger \textsc{Taggit} developed by B. B.
Greene \etal{} in 1971, used for an initial tagging of the Brown
Corpus (BC), a lot of effort has been devoted to improving the quality
of the tagging process in terms of accuracy and efficiency. Existing
taggers can be classified into three main groups according to the kind
of knowledge they use: linguistic, statistic and machine-learning
family. Of course some taggers are difficult to classify into these
classes and hybrid approaches must be considered. Within the
linguistic approach most systems codify the knowledge involved as a
set of rules (or constraints) written by linguists. The linguistic
models range from a few hundreds to several thousand rules, and they
usually require years of labor.

The most extended approach nowadays is the statistical family
(obviously due to the limited amount of human effort involved).
Basically it consists of building a statistical model of the language
and using this model to disambiguate a word sequence. The language
model is coded as a set of co-occurrence frequencies for different
kinds of linguistic phenomena. This statistical acquisition is usually
found in the form of n-gram collection, that is, the probability of a
certain sequence of length $n$ is estimated from its occurrences in
the training corpus. In the case of POS tagging, usual models consist
of tag bi-grams and tri-grams (possible sequences of two or three
consecutive tags, respectively). Once the n-gram probabilities have
been estimated, new examples can be tagged by selecting the tag
sequence with highest probability. This is roughly the technique
followed by the widespread Hidden Markov Model taggers. Although the
form of the model and the way of determining the sequence to be
modeled can also be tackled in several ways, most systems reduce the
model to unigrams, bi-grams or tri-grams. Moreover, some taggers try
to reduce the amount of training data needed to estimate the model,
and use the Baum-Welch re-estimation algorithm. Other works that can
be placed in the statistical family performs energy-function
optimization using neural nets. Recent works in this field try not to
limit the model to a fixed order n-gram by combining different order
n-grams, morphological information, long-distance n-grams, or
triggering pairs. These are some approaches that we may see
incorporated to POS tagging tasks in the short term.

Although the statistical approach involves some kind of learning,
supervised or unsupervised, of the parameters of the model from a
training corpus, we place in the machine learning family only those
systems that include more sophisticated information than a n-gram
model. Many approaches were explored in literature: some works tried
to automatically learn a set of transformation rules which best repair
the errors committed by a most-frequent-tag tagger. Others acquire
Constraint Grammar rules from tagged corpora or apply instance-based
learning. Promosing approaches explore decision trees induced from
tagged corpora, which are exploited, together with other statistical
and linguistic information, in a hybrid environment that applies
relaxation techniques over a set of constraints.

\subsection{POS tags}

POS tags were originially defined in \cite{marcus1993building} on the
Penn Treebank corpus as a comprehensive list of tags for words,
including noun, adjective, verb tenses and also symbols. The
Fig.~\ref{fig:pos-tags} shows the tags along with their description.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{figures/pos-tags.png}
  \caption[POS tags]{POS tags of the Penn Treebank tag set \cite{marcus1993building}}
  \label{fig:pos-tags}
\end{figure}

During years, many different annotation for standard of POS tagging
and dependency structure were developed, such us Stanford Dependencies
\cite{de2006generating, de2008stanford, silveira2014gold}, Google
universal part-of-speech tags \cite{lin2012syntactic} and the Interset
interlingua \cite{zeman2008reusable} for morphosyntactic tagsets.
Universal Dependencies (UD) \cite{nivre2016universal,
nivre2017universal} is a recent effort to achieve cross-linguistic
consistency of annotation, while still permitting language-specific
extensions when necessary. The UD annotation scheme uses a
representation in the form of dependency trees as opposed to a phrase
structure trees and makes available over $100$ treebanks for more than
$70$ languages. New Universal POS tags are listed in
Fig.~\ref{fig:upos-tags}.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/upos-tags.png}
  \caption[Universal POS tags]{Universal part-of-speech tags (UPOS) \cite{nivre2017universal}}
  \label{fig:upos-tags}
\end{figure}
 
\subsection{NLP Tools}

During years many good NLP tools were developed. The reasons are
twofold: first of all the availability of open-source dataset
simplified the training of such complex system, and secondly, NLP has
applications in many fields, from research to industry and even
government, thus, involving high demand for accurate and efficient
tools. Most famous and widely adopted appliances are CoreNLP
\cite{manning2014stanford}, \textsc{Flair} \cite{akbik2019flair},
spaCy\footnote{\href{https://spacy.io/}{https://spacy.io/}}, UDPipe
\cite{straka2018udpipe} and Stanza \cite{qi2020stanza}. Below we
discuss most relevant projects, highlighting their pro and cons. For a
detailed comparison between some NLP tools please refer to
\cite{schmitt2019replicable}.

\subsubsection{Standford CoreNLP}

Standford CoreNLP \cite{manning2014stanford} is the first NLP tool
developed for the mass adoption. CoreNLP was born at Standford
University in 2006 for internal usage and the released under an
open-source license in 2010. It implements a generic NLP pipeline from
tokenization through to coreference resolution. The motivations under
its development are:
\begin{itemize}
  \item To be able to quickly and painlessly get linguistic
  annotations for a text.
  \item To hide variations across components behind a common API.
  \item To have a minimal conceptual footprint, so the system is easy
  to learn.
  \item To provide a lightweight framework, using plain Java objects
  (rather than something of heavier weight, such as XML or UIMA's
  Common Analysis System (CAS) objects).
\end{itemize}
It tries to overcome problems of existing systems such us UIMA
\cite{ferrucci2004uima} and GATE \cite{cunningham2002gate} by allowing
the user to get ready in few minutes only knowning a little Java. For
example, Standford CoreNLP makes available an intuitive command line
interface and an easy way to write text annotators. Moreover, it
provides a simple concrete API which enables its usage from extenal,
existing systems. However, it does not attempt to provide multiple
machine scale-out (though it does provide multi-threaded processing on
a single machine).

Out of the box, Standford CoreNLP comes with many annotators and
supports all language encodings and various human languages like
English, Cinese, German, Franch and Arabic.

\subsubsection{Flair}

\textsc{Flair} \cite{akbik2019flair} is an NLP framework designed to
facilitate training and distribution of state-of-the-art sequence
labeling, text classification and language models. The core idea of
the framework is to present a simple, unified interface for
conceptually very different types of word and document embeddings.
This effectively hides all embedding-specific engineering complexity
and allows researchers to ``mix and match'' various embeddings with
little effort. The framework also implements standard model training
and hyperparameter selection routines, as well as a data fetching
module that can download publicly available NLP datasets and convert
them into data structures for quick set up of experiments.
\textsc{Flair} also comes with a collection of pre-trained models to
allow researchers to use state-of-the-art NLP models in their
applications.

\subsubsection{SpaCy}

SpaCy defines itself an ``Industrial-Strength Natural Language
Processing Toolkit''. It is a free, open-source library for advanced
Natural Language Processing (NLP) in Python. It is designed
specifically for production use and helps you build applications that
process and ``understand'' large volumes of text. It can be used to
build information extraction or natural language understanding
systems, or to pre-process text for deep learning.

SpaCy implements many core features like tokenization, POS tagging,
dependency parsing, lemmatization, Sentence Boundary Detection (SBD),
Named Entity Recognition (NER), Entity Linking (EL), similarity, text
classification and rule-based matching. Pratically, it is a swiss-army
knife for NLP pipelines.

\subsubsection{Stanza}

Stanza \cite{qi2020stanza} is a Python natural language
processing toolkit supporting many human languages. Its core features are:
\begin{itemize}
  \item From raw text to annotations. It features a fully neural
  pipeline which takes raw text as input, and produces annotations
  including tokenization, multi-word token expansion, lemmatization,
  part-of-speech and morphological feature tagging, dependency
  parsing, and named entity recognition.
  \item Multilinguality. Its architectural design is language-agnostic
  and data-driven, which allows us to release models supporting 66
  languages, by training the pipeline on the Universal Dependencies
  (UD) treebanks and other multilingual corpora
  \item State-of-the-art performance. They evaluate Stanza on a total
  of 112 datasets, and find its neural pipeline adapts well to text of
  different genres, achieving state-of-the-art or competitive
  performance at each step of the pipeline-
\end{itemize}
Additionally, Stanza features a Python interface to the widely used
Java CoreNLP package, allowing access to additional tools such as
coreference resolution and relation extraction. It is fully open
source and mdeks available pretrained models for all supported
languages and datasets available for public download.

\section{Recurrent Neural Network}
\label{sec:rnn}

A recurrent neural network (RNN) is a special type of an artificial
neural network (ANN) adapted to work for time series data or data that
involves sequences. Ordinary feed forward neural networks are only
meant for data points independent of each other while RNN introduces
the concept of ``memory'' that helps them store the states or
information of previous inputs to generate the next output of the
sequence by sharing weights.

An ANN with recursion can be easily modeled by the Eq.\ref{eq:rnn}:

\begin{equation}
  \label{eq:rnn}
  \bm{h}^{(t)} = f ( \bm{h}^{(t - 1)}, \bm{x}^{(t)} ; \bm{\theta} ),
\end{equation}

where $\bm{x}^{(1)}, \ldots, \bm{x}^{(\tau)}$ is the input sequence,
$f$ is the function that maps hidden units at time step $t - 1$ to
time step $t$ and $\bm{\theta}$ the parameters of $f$. The equation is
recurrent because the deﬁnition of $\bm{h}$ at time $t$ refers back to
the same deﬁnition at time $t - 1$. The process is visually depicted
in Fig.\ref{fig:rnn-with-unfold} \emph{(Left)} where the black square
indicates a delay of a single time step. At this point the problem is:
how do we propagate our input throught the RNN? Here comes in the idea
of unfolding. To unfold an RNN basically means to unroll the cyclic
circuit in a graph by explicitly applying the function $f$ on
$\bm{x}^{(t)}$, the input at time $t$, and $\bm{h}^{(t-1)}$, the
hidden state at time $t - 1$. RNN unfolding is visually explained in
Fig.\ref{fig:rnn-with-unfold} \emph{(Right)}.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{figures/rnn-with-unfold.png}
  \caption[Recurrent network circuit diagram and unfolded computational graph]{
    A recurrent network with no outputs. This recurrent network just
    processes information from the input $x$ by incorporating it into
    the state $\bm{h}$ that is passed forward through
    time.\cite{goodfellow2016deep} \textit{(Left)} Circuit diagram.
    The black square indicates a delay of a single time step.
    \textit{(Right)} The same network seen as an unfolded
    computational graph, where each node is now associated with one
    particular time instance.
  }
  \label{fig:rnn-with-unfold}
\end{figure}

Tipycal RNNs add extra architectural features such as output layers
that read information out of the state $\bm{h}$ to make predictions.

When the recurrent network is trained to perform a task that requires
predicting the future from the past, the network typically learns to
use $\bm{h}^{(t)}$ as a kind of lossy summary of the task-relevant
aspects of the past sequence of inputs up to t. This summary is in
general necessarily lossy, since it maps an arbitrary length sequence
$( \bm{x}^{(t)}, \bm{x}^{(t-1)}, \bm{x}^{(t-2)} , \ldots ,
\bm{x}^{(2)}, \bm{x}^{(1)} )$ to a ﬁxed length vector $\bm{h}^{(t)}$ .
Depending on the training criterion, this summary might selectively
keep some aspects of the past sequence with more precision than other
aspects. For example, if the RNN is used in statistical language
modeling, typically to predict the next word given previous words, it
may not be necessary to store all of the information in the input
sequence up to time $t$, but rather only enough information to predict
the rest of the sentence.

Recurrent neural networks can be built in many diﬀerent ways with
varying architecture. In literature emerged some successful RNNs
architecture with a well defined and non-casual structure, such us
Bidirection RNN (Bi-RNN), Long Short-Term Memory (LSTM) and Gated
Recurrent Unit (GRU). Each RNN architecture has some pro and cons and
can be applied successfully to specific tasks.

\subsection{Bidirectional RNN}

As the name suggests, bidirectional RNNs combine an RNN that moves
forward through time beginning from the start of the sequence with
another RNN that moves backward through time beginning from the end of
the sequence. Those networks have been applied successfully when the
output we want to predict depetnds on the whole input sequence. For
example, in speech recognition, the correct interpretation of the
current sound as a phoneme may depend on the next few phonemes because
of co-articulation and potentially may even depend on the next few
words because of the linguistic dependencies between nearby words: if
there are two interpretations of the current word that are both
acoustically plausible, we may have to look far into the future (and
the past) to disambiguate them. This is also true of handwriting
recognition and many other sequence-to-sequence learning tasks such us
bioinformatics.

The Bi-RNNs convoy an intrisic advantage and drawback at the same
time: it requires the full sequence, so it cannot work with real-time
input sequences, but for the same reason it can better infer the
output from the past and future kwown context.

\subsection{Gated RNN (LSTM and GRU)}
\label{subsec:gated-rnn}

Being able to work with long term dependencies is crucial for many
applications: think, for example, to a dialogue system which should
remember previous parts of the speech on order to keep on with the
dialogue.

Unfortunately, RNNs and long term dependencies yield a mathematical
challenge. The basic problem is that gradients propagated over many
stages tend to either vanish (most of the time) or explode (rarely,
but with much damage to the optimization). Moreover, I. Goodfellow
\etal{} in \cite{goodfellow2016deep}, states also that:

\begin{quote}
  Even if we assume that the parameters are such that the recurrent
  network is stable (can store memories, with gradients not
  exploding), the diﬃculty with long-term dependencies arises from the
  exponentially smaller weights given to long-term interactions
  (involving the multiplication of many Jacobians) compared to
  short-term ones.
\end{quote}

One way to deal with long-term dependencies is to design a model that
operates at multiple time scales, so that some parts of the model
operate at ﬁne-grained time scales and can handle small details, while
other parts operate at coarse time scales and transfer information
from the distant past to the present more eﬃciently. Various
strategies for building both ﬁne and coarse time scales are possible.
These include the addition of skip connections across time, ``leaky
units'' that integrate signals with diﬀerent time constants, and the
removal of some of the connections used to model ﬁne-grained time
scales.

Like leaky units, gated RNNs are based on the idea of creating paths
through time that have derivatives that neither vanish nor explode.
Leaky units did this with connection weights that were either manually
chosen constants or were parameters. Gated RNNs generalize this to
connection weights that may change at each time step.

Leaky units allow the network to accumulate information (such as
evidence for a particular feature or category) over a long duration.
However, once that information has been used, it might be useful for
the neural network to forget the old state. For example, if a sequence
is made of sub-sequences and we want a leaky unit to accumulate
evidence inside each sub-subsequence, we need a mechanism to forget
the old state by setting it to zero. Instead of manually deciding when
to clear the state, we want the neural network to learn to decide when
to do it.

LSTM basically integrates the idea of self-loops to produce paths
where the gradient can ﬂow for long durations with the addition to
control weight on this self-loop conditioned on the context, rather
than ﬁxed. By making the weight of this self-loop gated (controlled by
another hidden unit), the time scale of integration can be changed
dynamically. LSTM recurrent networks have ``LSTM cells'' that have an
internal recurrence (a self-loop), in addition to the outer recurrence
of the RNN. Fig.~\ref{fig:lstm} illustrate the LSTM block diagram. In
an LSTM cell, the self-loop controls the flow of information from
input to output through three key components: a forget gate, an
external input gate and an output gate.

\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{figures/lstm.png}
  \caption[Block diagram of the LSTM recurrent network cell]{
    Block diagram of the LSTM recurrent network ``cell''
    \cite{goodfellow2016deep}. Cells are connected recurrently to each
    other, replacing the usual hidden units of ordinary recurrent
    networks. An input feature is computed with a regular artiﬁcial
    neuron unit. Its value can be accumulated into the state if the
    sigmoidal input gate allows it. The state unit has a linear
    self-loop whose weight is controlled by the forget gate. The
    output of the cell can be shut oﬀ by the output gate. All the
    gating units have a sigmoid nonlinearity, while the input unit can
    have any squashing nonlinearity. The state unit can also be used
    as an extra input to the gating units. The black square indicates
    a delay of a single time step.
  }
  \label{fig:lstm}
\end{figure}

Along with LSTM architecture, a simpler version was recently proposed
with the aim to simplify cells and speed-up computations: the gated
recurrent units (GRU). The behaviour wrt LSTM is very similar, the
only difference is that a single gating unit simultaneously controls
the forgetting factor and the decision to update the state unit.

\section{Object Detection and Recognition Systems}
\label{sec:object-detection-recognition}

Object detection (OD) and object recognition (OR) systems are crucial
in a wide variety of everyday tasks such us face detection, image and
video databases information retrieval, surveillance applications,
driver assistance, self drive, robotics, automation and, specially in
vision tasks, it is a foundamental building block used to extract
information from images.

The primary essence of those systems can be broken down into two
parts: to locate objects in a scene such us by drawing a bounding box
around the object (object detection) and later to classify the objects
based on the classes it was trained on (obejct recognition). OD and OR
are Often used together and for this reason the name of two tasks is
usually used interchangeably. As for visual grounding
(Sec.~\ref{sec:two-stage-vs-one-stage}), we can group the
state-of-the-art detection systems in two main approaches: one stage
methods (YOLO -- You Only Look Once \cite{redmon2016you}, SSD -- Single
Shot Detection \cite{liu2016ssd}) and two stage methods (R-CNN
\cite{girshick2014rich}, Fast R-CNN \cite{girshick2015fast}, Faster
R-CNN \cite{ren2015faster}). 

In the following sections, we briefly summarize those methods
discussing their main idea nd highlighting pro and cons.

\subsection{YOLO -- \emph{You Only Look Once}}
\label{subsec:yolo}

YOLO \cite{redmon2016you} is a state-of-the-art, real-time object
detection system which offers extreme levels of speed and accuracy.
YOLO reframes object detection as a regression problem to spatially
separated bounding boxes and associated class probabilities. A single
neural network predicts bounding boxes and class probabilities
directly from full images in one evaluation. Since the whole detection
pipeline is a single network, it can be optimized end-to-end directly
on detection performance. This architecture offers some advantages wrt
to other methods: first, YOLO is extremely fast and second, it reasons
globally about the image when making prediction implicitly encoding
contextual information about classes as well as their appearance. Last
but not least, YOLO is more likely to learn generalizable
representations of objects because of its global reasoning on image.

\subsection{SSD -- \emph{Single Shot Detection}}

SSD \cite{liu2016ssd} is another state-of-the-art method for detecting
objects in images using a single deep neural network. SSD discretizes
the output space of bounding boxes into a set of default boxes over
different aspect ratios and scales per feature map location and, at
prediction time, the network generates scores for the presence of each
object category in each default box and produces adjustments to the
box to better match the object shape. Additionally, the network
combines predictions from multiple feature maps with diﬀerent
resolutions to naturally handle objects of various sizes. Also, SSD
offers high speed performance due its single network and for this
reason it is easy to train and straightforward to integrate into
systems that require a detection component. In terms of accuracy,
experimental results conﬁrm that SSD has competitive accuracy also in
low resolution compared to methods that utilize an additional object
proposal step.

\subsection{R-CNN -- \emph{Regions with CNN}}

R-CNN \cite{girshick2014rich} is the first performant object detection
system ever built. It is composed by a two-stage architecture: in the
first step the Selective Search algorithm generates around 2000
category-independent region proposals for the input image, in the last
step instead it extracts a fixed-length feature vector from each
proposal using a CNN (hence the name R-CNN), and then classifies each
region with category-specific linear SVMs. The method shows
interesting results and can be applied also with scarce data
availability: the system can be pretrained and the fine-tuned. In
temrs of computation time, R-CNN performs some expensive operations
required for greedy non-maximum suppression, amogn others, but on
relatively small inputs.

\subsection{Fast R-CNN}

The Fast R-CNN \cite{girshick2015fast} model was built to counter a
few drawbacks of the previous R-CNN model. In this approach, similar
to the previous, Selective Search is used to generate region proposals
but the input image is fed to a CNN and a convolutional feature map is
generated from it which is then used to identify the regions and
combine them into larger squares by using a RoI pooling layer. A
softmax layer is finally used to predict the class of the proposed
region.

\subsection{Faster R-CNN}
\label{subsec:faster-rcnn}

Unlike R-CNN and Fast R-CNN, Faster R-CNN \cite{ren2015faster} does
not use Selective Search which is a slow process. Instead, it allows
the network to learn the region proposals throught a separate network,
able to predict the region proposals. The predicted proposals are then
pooled into larger squares using the RoI pooling layer which is then
finally used to classify the image.

\section{Word Embeddings}
\label{sec:word-embeddings}

Being able to model and represent natural language features is a
relevant machine learning task that belongs to the natural language
processing field and has many applications, such us information
retrieval \cite{sanderson2010christopher}, document classification
\cite{sebastiani2002machine}, question answering
\cite{tellex2003quantitative}, named entity recognition
\cite{turian2010word}, and parsing \cite{socher2013parsing}.

Instead of treating words as atomic units where there is no notion of
similarity between words, as these are represented as indices in a
vocabulary, natural language information can be represented as
real-valued feature vectors throught a semantic vector space, where
representation of words are learned for a predefined fixed sized
vocabulary from a corpus of text and the intrinsic quality of such a
set of word is evaluated by the distance or angle between pairs of
word vectors representations or by their various dimensions of
difference\cite{mikolov2013linguistic}. In light of this we can
formulate the definition for word embedding: 

\begin{quote}
  \textit{A word embedding is a learned representation for text where
  words that have the same meaning have similar representation.}
\end{quote}

With the years, literature explored different approaches for learning
good words representation: either joint with the neural network model
on some task, such as document classification, or using document
statistics throught unsupervised settings such us Latent Semanti
Analysis. Both families suffer significant drawbacks. While methods
like LSA efficiently leverage statistical information, they do
relatively poorly on the word analogy task, indicating a sub-optimal
vector space structure. Methods like skip-gram may do better on the
analogy task, but they poorly utilize the statistics of the corpus
since they train on separate local context windows instead of on
global co-occurrence counts.

\subsection{Indexing by Latent Semantic Analysis}

A first approach for learning representative word embeddings was first
introduced by S. Deerwester \etal{} in \cite{deerwester1990indexing},
exploiting global matrix factorization by means of latent semantic
analyis (LSA). The proposed approach tries to overcome the
deficiencies of term-matching retrieval by treating the unreliability
of observed term-document association data as a statistical problem.
Indeed, they take advantage of a particular version of LSA that uses
singular-value decomposition. First, a large matrix of term-document
association data is crteated and then a semantic space wherein terms
and documents that are closely associated are placed near one another
is constructed. Singular-value decomposition allows the arrangement of
the space to reflect the major associative patterns in the data, and
ignore the smaller, less important influences.

\subsection{Word2Vec}

Word2Vec, introduced by T. Mikolov in \cite{mikolov2013efficient} and
then updated and revisioned in \cite{mikolov2013distributed,
mikolov2013linguistic}, is a statistical method for efficiently
learning a standalone word embedding from a text corpus. One of the
goal of Word2Vec is to learn distributed representations of words by
neural networks, in particular, they introduced two different learning
models that can be used as part of the word2vec approach to learn the
word embedding: Continuous ag-of-Words (CBOW) and Continuous Skip-Gram
Model. The CBOW model learns the embedding by predicting the current
word based on its context. The continuous skip-gram model learns by
predicting the surrounding words given a current word. Both models,
shown in Fig.~\ref{fig:word2vec-learning-models} are focused on
learning about words given their local usage context, where the
context is defined by a window of neighboring words. This window is a
configurable parameter of the model. The quality of these
representations is measured in a
word similarity task.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/word2vec-training-models.png}
  \caption[Graphical representation of the CBOW model and Skip-gram model]{
    Graphical representation of the CBOW model and Skip-gram model
    \cite{mikolov2013exploiting}. In the CBOW model, the distributed
    representations of context (or surrounding words) are combined to
    predict the word in the middle. In the Skip-gram model, the
    distributed representation of the input word is used to predict
    the context.
  }
  \label{fig:word2vec-learning-models}
\end{figure}

The key benefit of the approach is that high-quality word embeddings
can be learned efficiently (low space and time complexity), allowing
larger embeddings to be learned (more dimensions) from much larger
corpora of text (billions of words). Also, Word2Vec perform
significantly better than LSA for preserving linear regularities among
words and LDA moreover becomes computationally very expensive on
large data sets.

\subsection{GloVe}

GloVe is an extension to the Word2Vec method for efficiently learning
word vectors, developed by J. Pennington \etal{} in
\cite{pennington2014glove}, that wants to to marry both the global
statistics of matrix factorization techniques like LSA with the local
context-based learning in Word2Vec. Rather than using a window to
define local context, GloVe constructs an explicit word-context or
word co-occurrence matrix using statistics across the whole text
corpus. The result is a learning model that may result in generally
better word embeddings.

\subsection{Other methods}

\newthought{BERT} -- Bidirectional Encoder Representations from
Transformers is a relatively recent language representation model,
introduced by J. Devlin \etal{} in \cite{devlin2018bert}, designed to
pretrain deep bidirectional representations from unlabeled text by
jointly conditioning on both left and right context in all layers. The
underlying technology is a deep transformer istead of a traditional
recurrent neural network.

As outlined in \cite{gupta2020differences}, BERT has many difference
wrt traditional word embeddings: for example, it differs from Word2Vec
or GloVe because it is context dependent. More precisely, BERT
produces multiple vector representations for the same word, based on
the context in which the word is used. For example, in the sentences
``We went to the river bank'' and ``I need to go to bank to make a
deposit'', the word \textit{bank} produces different embeddings in
BERT for each sentence by taking into account the context, while in
Word2Vec or Glove, such word is represented by a single, unique 
vector. Another key difference is that BERT explicitly takes as input
the position (index) of each word in the sentence before calculating
its embedding, i.e., it consider also the position of the word in
sentece. Technically speaking, another difference is that since BERT
generates contextual embeddings, the input to the model is a sentence
rather than a single word. This is because the BERT model needs to
know the context or the surrounding words before generating a word
vector. Of course, BERT model can be used to get pre-computed static
single-word vectors (similar to Word2Vec or GloVe), but it defeats the
purpose of having contextualized embeddings.

\section{Similarity Measures}

\subsection{Intersection over Union (IoU)}
\label{subsec:iou}

Given a pair of bounding box coordinates $(\bm{b}_i , \bm{b}_j)$, the
Intersection over Union (IoU), also known as Jaccard index, i.e.,
coefficient of similarity for two sets of data
\cite{jaccard1901comparative}, is an evaluation metric used mainly in
object detection tasks, which aims to evaluate how much the two
bounding box refer to the same content in the image
\cite{rigoni2021better, padilla2020survey}. Specifically, it is
defined as:
\[
  \iou(\bbox_i, \bbox_j) = \frac{| \bbox_i \inters \bbox_j |}{| \bbox_i \union \bbox_j |}
\]
where $| \bbox_i \inters \bbox_j |$ is the area of the box obtained by
the intersection of boxes $\bbox_i$ and $\bbox_j$ , while $| \bbox_i
\union \bbox_j |$ is the area of the box obtained by the union of
boxes $\bbox_i$ and $\bbox_j$. A conceptual explaination is given in
Fig.~\ref{fig:iou}. It is invariant to the bounding boxes sizes, and
it returns values that are strictly contained in the interval $[0, 1]
\in \Rset$, where $1$ means that the two bounding boxes refer to the
same image area, while a score of $0$ means that the two bounding
boxes do not overlap at all.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/iou.png}
  \caption[Intersection over Union (IoU)]{Intersection over Union (IoU) \cite{padilla2020survey}}
  \label{fig:iou}
\end{figure}

\subsection{Pointing Game Metric}
\label{subsec:pointing-game-metric}

The pointing game can be considered an human evaluation metric
\cite{petsiuk2018rise} and was first introduced by J. Zhang \etal{} in
\cite{zhang2018top}. Given $(\bm{b}_i , \bm{b}_j)$ a pair of bounding
box, the ponting game metric defines a hit the highest saliency point
of $\bm{b}_i$ lies inside the $\bm{b}_j$, otherwise it is a miss.

The highest saliency point of a bounding box is usually defined
depending on the task and setting of the problem. For example, in the
phrase localization task, such point is the center of the bounding
box. In other cases, given an attention mask which localizes an object
in an image, the highest saliency point is the maximum point on the
attention mask.

\subsection{Cosine Similarity}

Cosine similarity is a metric used to measure how similar
representations are irrespective of their size (norm). Mathematically,
it measures the cosine of the angle between two vectors projected in a
multi-dimensional space:
\begin{equation}
  s_{\cos}(\bm{a}, \bm{b}) = \frac{\veca \cdot \vecb}{\normtwo\veca \normtwo\vecb},
\end{equation}
where $\veca \cdot \vecb$ is the dot product between two vectors and
$\normtwo\veca$ is the L2-norm of a vector.
