%!TEX root = ../dissertation.tex
\chapter{Conclusion}
\label{conclusion}

In this work we focused on the problem of phrase grounding with weak
supervision. The phrase grounding task is relevant for many other
import tasks, as outlined in Ch.~\ref{ch:intro}, such us visual
question answering, image retrieval, robotic navigation and more. The
complexity of phrase grounding problem with weak supervision is
partially due to its multimodal nature that involves textual and
visual features, which, in turn, brings extra challenges as disccused
in Ch.~\ref{ch:background}, and partially because the missing of
ground truth which makes unfeasible the learning throught standard
approaches. Being able to solve the pharse grounding problem under
weakly supervised settings is extremely important both because we can
overcome the bottleneck in collecting new and larger dataset which can
improve even more grounding performance and it would constitute a
strong baseline for fully supervised settings. At the light of this,
we discussed in Ch.~\ref{ch:model} how to solve the problem thanks to
free information offered by the object detector: a probability
distribution over given set of labels for bounding box proposals. We
integrated this form of prior knowledge in our model as a similarity
score between the concept in query and the categorized content of a
proposal by empowering the score of query related proposals while
downscale other proposals' score. Finally, the model leanrs to
maximize the multumodal similarity between query and proposals
belonging to positive example, i.e., sentence with its own image,
while minimizing the multimodal similarity on negative examples. We
shown in Ch.~\ref{ch:experiments} that our approach is very promising
and outperforms all previous approach except the excellent work by Q.
Wang \etal{} \cite{wang2020maf}. In Sec.~\ref{TODO} we motivated our
deficiency wrt their work. \todo{make the sec.}

\section{Encountered Problems}

During this work we encountered many problems, both conceptual and
thecnical. Not surprisingly, the trickies one was conceptual: we
thought we could shift from weak supervision to unsupervision by means
of concept similarity. We had just found out the (super)power of
concept similarity, but, of course, not all that glitters is gold.
More details are reported in Appendix~\ref{TODO}\todo{MAKE APPENDIX}.

Another problem we encountered is a flaw in phrase grounding field of
study: we found some publication to be uncomplete and not professional
as other works on mainstream tasks. In particular, we noticed the
inconsistency between the naming, task and problem definition.
Moreover, many works do not allow the reproduction of results by
omitting important implementation details: one a few works made their
implementation available online. Furthermore, some works perform
unfair comparison by mixing up different backbones for object detector
or different word embeddings, or, even worse, they ignore works with
good results. For the sake of science, we hope that this situation
will be resolved as soon as possible.

Many other problems instead were technical. As already pointed out,
phrase grounding requires a strong knowledge of many components: from
object detectors to word embeddings, from LSTM to similarity measures.
Hence, also a solid technical skill is required to make the components
interact each others, develop a maintainable codebase and implement
the model with fewer bugs possibile. For this reason we devoted a good
amount of effort in developing high-quality code, making use of main
guidelines and principles from software engineering. Indeed, our model
is covered by $137$ unit tests and $8$ integration tests.
