%!TEX root = ../dissertation.tex
\chapter{Conclusion}
\label{ch:conclusion}

In this work we focused on the problem of phrase grounding with weak
supervision. The phrase grounding task is relevant for many other
import tasks, as outlined in Ch.~\ref{ch:intro}, such as visual
question answering, image retrieval and robotic navigation. The
complexity of phrase grounding problem with weak supervision is
partially due to its multimodal nature that involves textual and
visual features, which, in turn, brings extra challenges related to
components discussed in Ch.~\ref{ch:background}, and partially because
the missing of ground truth which makes unfeasible the learning
through standard approaches. Being able to solve the phrase grounding
problem under weakly supervised settings is extremely important both
because we can overcome the bottleneck in collecting new and larger
dataset which can improve even more grounding performance and it would
constitute a strong baseline for fully supervised settings. At the
light of this, we discussed in Ch.~\ref{ch:model} how to solve the
problem thanks to free information offered by the object detector: a
probability distribution over given set of labels for bounding box
proposals. We integrated this form of prior knowledge in our model as
a similarity score between the concept in query and the categorized
content of a proposal by empowering the score of query related
proposals while downscale other proposals' score. Finally, the model
learns to maximize the multimodal similarity between query and
proposals belonging to positive example, i.e., sentence with its own
image, while minimizing the multimodal similarity on negative
examples. We shown in Ch.~\ref{ch:experiments} that our approach is
very promising and on Flickr30k Entities test set outperforms all
previous approaches except the excellent work by Q. Wang \etal{}
\cite{wang2020maf}. In Sec.~\ref{sec:results} we motivated our
deficiency wrt their work.

As a take home message I would say that research is also made of
failures: sometimes what seems a good idea may reveals inefficient or
erroneous, the fun fact is that you cannot know it before trying. But
science is made also of failures, and we must take out the best from
them.

\section{Encountered Problems}

A problem we encountered during our researches is a flaw in phrase
grounding field of study: we found some publications to be incomplete
and not professional as other works on mainstream tasks. In
particular, we noticed the inconsistency between the naming, task and
problem definition. Moreover, many works do not allow the reproduction
of results by omitting important implementation details: a few works
made their implementation available online. Furthermore, some works
perform unfair comparisons or ignore last published results. For the
sake of science, we hope that this situation will be resolved as soon
as possible.

Many other problems instead were technical. As already pointed out,
phrase grounding requires a strong knowledge of many components: from
object detectors to word embeddings, from LSTM to similarity measures.
Hence, also a solid technical skill is required to make the components
interact each others, develop a maintainable codebase and implement
the model with fewest bugs possible. For this reason we devoted a good
amount of effort in developing high-quality code, making use of main
guidelines and principles from software engineering. Indeed, our model
is covered by more than $130$ unit tests and $8$ integration tests.

\section{Future Development}

The prediction component we adopted is a simple similarity function
between vectors where similar vectors obtain higher scores. Thus, in
the end the model is forced to learn representation of visual features
that mimic textual features latent space (and vice versa) proportional
to the similarity score. Such forced features representation may be
unnatural and could be improved. For example
\cite{datta2019align2ground} considers an encoder-decoder architecture
for features fusion, while other works employ an attention mechanism.
Carefully choosing such tool may lead to better performance. On the
same way, also for text we work with relatively simple and base tools.
For word embeddings a good, recent alternative wrt what we used
(Word2Vec) is BERT. For the encoding of sentences we used an RNN with
LSTM cells, however, recent transformer architectures show promising
performance in this task an may be successfully integrated also in a
grounding system.

There is also a large margin of improvement in our loss. First of all,
to consider more negative examples as done in \cite{wang2020maf} may
reduce the time needed for assimilating a representation and to learn
robust and more discriminative features. Moreover, the multimodal
similarity function $\fmmsim$, defined in Sec.~\ref{sec:loss}, is
defined by the maximum score over proposals. An improvement that may
lead to interesting results is to consider not only the proposal with
maximum score per query, but instead to take into account all
proposals score in a weighted manner.

Another interesting deepening regards a strategy for disambiguating
similar proposals. Some object detectors (as the one we adopted, see
Sec.~\ref{sec:impl-details}) also return a probability distribution on
a set of attributes, highlighting extra information over proposals. A
good improvement would be to join the information from adjectives in
queries with attributes over proposals and up or down weight scores
based on this attribute similarity measure. Such information may be
precious to discriminate between similar proposals.
