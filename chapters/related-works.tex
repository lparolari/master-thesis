%!TEX root = ../dissertation.tex
\begin{savequote}[75mm]
Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
\qauthor{Quoteauthor Lastname}
\end{savequote}

\chapter{Related works}

\section{Phrase Grounding}

Phrase grounding is the task that studies the mapping from noun
phrases to regions of an image (Sec.~\ref{sec:visual-grounding}) and
requires strong understanding of both visual and textual modalities,
specially when not all ground truth is available. 

Even if phrase groudning is an extremely relevant task, the study of
this problem in literature starts relatively late, mostly because a
community-accepted formalization of the problem took its time for
being developed. First efforts in this direction can be identified by
the work of S. Fidler \etal{} \cite{fidler2013sentence}. They
developed a model for semantic understanding of scene exploiting both
textual and visual information. In their work, short sentences are
parsed into nouns and prepositions, which are used to generate
potentials in a holistic scene model. An attempt to find an alignment
between entities in image and sentence is done by C. Kont \etal{}
\cite{kong2014you}. They proposed a model that exploits potentials
computed from text and RGB-D imagery to reason about the class of the
3D objects, the scene type, as well as to align the nouns/pronouns
with the referred visual objects. They used complex sentences, but
grounding is contrained to nouns of 21 object classes relevant to
indoor scenes. R. Hu \etal{} \cite{hu2016natural} moved to a slightly
different task: natural language objects retrieval. They address the
task to localize a target object within a given image based on a
natural language query of the object. It differs from text-based image
retrieval task as it involves spatial information about objects within
the scene and global scene context. To address this issue, they
proposed a novel Spatial Context Recurrent ConvNet (SCRC) model as
scoring function on candidate boxes for object retrieval, integrating
spatial configurations and global scene-level contextual information
into the network. In particular, their model processes query text,
local image descriptors, spatial configurations and global context
features through a recurrent network, outputs the probability of the
query text conditioned on each candidate box as a score for the box,
and can transfer visual-linguistic knowledge from image captioning
domain to their task. In \cite{rohrbach2016grounding}, A. Rohrbach
\etal{} tackle the original problem of fine-grained phrase grounding.
They developed a model able to reconstruct textual features in an
encoder-decoder fashion, powered by an attention module which
implements localization of regions of interest. Here, the attention is
used to both ground and reconstruct. During training, the attention
learns how to reconstruct given sentence describing the region to
ground. During inference, attention localizes the such region. The key
motivation under this idea is that by training to reconstruct the text
phrase, the model learn first to ground the phrase in the image.
Moreover, their model is able to work with no, semi or full
supervision. When no ground truth is available, the model simply
learns to reconstruct the given sentence. Otherwise, the correct
localization is enforced besides the reconstruction.

 \todo{REWRITE papers below}

F. Xiao \etal{} in \todo{CITE
Weakly-supervised Visual Grounding of Phrases with Linguistic
Features} exploit linguistic structure arguing it can convoy rich
semantic. By focusing on parent-sibling and sibling-sibling linguistic
relations in a sentence, they build an attention map which can
localize phrases in image following linguistic rules. Also S. A. Javed
\etal{} in \todo{Learning Unsupervised Visual Grounding Through
Semantic Self-Supervision} make use of an attention map, but in this
case they optimize the model for concept learning proxy task, which
require the model to jointly decode the common concept in a batch of
examles composed by image and captions. S. Datta \textit{et al.} in
\todo{CITE: align2g} developed three-step model in which, 
\begin{enumerate*}[label=(\roman*)] 
    \item bounding boxes are filtered based on caption, i.e., only a
    box per caption is selected, then
    \item caption conditioned bounding boxes are encoded with an
    order-invariant deep encoder implemented as a two-layer MLP, and
    finally
    \item those encoded caption-conditioned bounding box are matched
with the caption.
\end{enumerate*}
Tha caption is required to match the encoded features as the model is
optimized for the caption-to-image retrieval proxy task. In \todo{CITE
Knowledge Aided Consistency for Weakly Supervised Phrase Grounding} K.
Chen \etal{} approach the problem by learning to reconstruct the
input. They reconstruct visual features throught a MLP and the textual
features throught an LSTM. Both two branches rely on a
filter/downweight network, namely Knowledge Aided Consistency (KAC)
network, which is responsible to discard the unrelated proposal. The
downweight network is implemented by considering the similarity of the
visual content identified by the label of a bounding box and the
aggregate of a phrase. In the field of weakly-supervised referring
expression grounding, X. Liu \etal{} in \todo{CITE adaptive net}
exploit discriminative information about subject, location and context
wheighted by an attention network and the used to reconstruct the
hidden features of subject, location and context, the input query and
attribute information.

J. Wang in \etal{} \todo{CITE phrase loc}, instead, tried a very new
and different approach. They developed a model able to work under
unsupervised settings, in which no ground truth is available at
training time. The model 
\begin{enumerate*}[label=(\roman*)] 
    \item detects instances by means of six heterogeneous object
    detector, which vary in terms of accuracy, number of classes,
    dataset trained on... then
    \item selects the most related concept with respect to the phrase
    by computing the similarity of detected instance labels and phrase
    words, after this
    \item localized the right bounding box which refers to the
    selected concept.
\end{enumerate*}
The model is not trained: in fact, they leverage the information that
object detector carry with them. A. Rohrbach \etal{} in \todo{CITE
grounding by rec} provide their version of phrase localization in
unsupervised settings together with semi and fully supervised settings
by developing the model \textit{GroundeR}. The model learns to
reconstruct the input phrase based on the boxes it attends to trought
an attention mechanism. 

In supervised settings, the scientific community devoted much effort
solving the phrase localization problem, leading to the development of
many solution making use of different strategies. For example, H.
Akbari \etal{} in \todo{CITE multilevel multimodal} employed a
multi-level multimodal common semantic space for both visual and
textual features in order to capture semantic information among the
two modalities. Z. Yang \etal{} in \todo{CITE a fast and acc one stage
appr} exploit YOLOv3 object detector features joined with textual
features in order to learn to ground wihtout requiring the proposal
extraction stage. In \todo{CITE zero shot groudning}, A. Sadhu \etal{}
on the same line of \todo{CITE a fast and acc one stage appr} propose
a one-stage model focusing on the slightly different task of Zero Shot
Grounding, which can include unseen nouns in phrases. Instead, D.
Rigoni \etal{} in \todo{CITE a better loss} optimize the model to
learn with a new loss involving the use of metrics like Intersection
Over Union (IoU) and Complete IoU (CIoU).

\newthought{Visual Textual Knowledge Entity Linking}, introduced in
\todo{CITE 10, 8, 9 da a new loss}, is a more complex task than phrase
localization: it requires an artificial agent to jointly recognize the
entities shown in the image and mentioned in the text, and to link
them to its prior background knowledge. The solution to the VTKEL
problem could lead to major scientific advancement towards a better
understanding of semantic information contained in the image and
textual sentence, respectively. In fact, the knowledge graph allows to
introduce semantic reasoning on the information contained in both the
image and the textual sentence, which could lead to innovative
solutions for the weakly supervised phrase localization problem and
for the partially annotated dataset problem.

\newthought{Visual Question Answering} (VQA) is a computer vision task
where a system is given a text-based question about an image, and it
must infer the answer \todo{CITE VQA future chal}. Questions can be
arbitrary and can encompass many computer vision task, varying from
object recognition or detection to attribute or scene classification
and also to counting. Beyond this, there are many more complex
question that can be asked, such as questions about spatial
relationships among objects and common sesnse reasoning question. 
Under this definition, phrase localization becomes a foundamental
building block for VQA systems.

\todo{ image retrieval ??? altro? }
